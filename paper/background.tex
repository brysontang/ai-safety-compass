\section{Background}

\subsection{Existing Safety Benchmarks and Gap}

% Briefly cover existing benchmarks and highlight your unique contribution.

\subsection{Safety and Risk}

The AI safety literature suggests that AI harbors many potential risks such as human extinction, large-scale societal disruption, and malicious uses, likening these risks to threats such as pandemics or nuclear war \citep{center_for_ai_safety_statement_2025, turchin_classification_2020}. Concerns include manipulating societal beliefs for political or fiscal gains, long-term plans where each step appears individually harmless but cumulatively results in serious consequences, and enabling sophisticated cyber or physical attacks \citep{shevlane_model_2023, ferrara_genai_2024}. Studies also illustrate AI's inability to manage common-pool resources sustainably without explicit alignment, suggesting inherent dangers even if each AI is safe in isolation \citep{perolat_multi_agent_2017, phelps_machine_2024}.

Alignment is the intuitive fix to these risks, however, alignment efforts themselves are controversial. Recent research highlights the vulnerabilities and paradoxes of alignment, notably methods that are used to align AI are detached from the ethics, meaning the powerful techniques inadvertently simplify efforts to create malicious AI \citep{zhou_emulated_2024, west_ai_2024}. This suggests that the act of understanding how to get AI to follow strict guidelines could help create the opposite effect, increasing risk, suggesting a critical trade-off of strong alignment techniques.

The open-source debate emphasizes transparency to rigorously test and work together to solve safety risks in AI \citep{horowitz_ai_2021}. Furthermore, universal jailbreaks that allow the user to query closed-source models for dangerous content are openly shared, putting into question if closed-sourcing AI is even a viable strategy to stopping dangerous content \citep{elderplinius2025l1b3rt4s, zou_universal_2023}. Although, open-source models are still capable of generating harmful content, \cite{vidgen_simplesafetytests_2024} demonstrate that open-sourced models fail simple safety tests up to 27\% of the time, underscoring the risks inherent in open distribution. Open-sourcing AI also encourages global teamwork instead of an arms race \citep{horowitz_ai_2021}. On the contrary, proponents of closed-source AI argue that by keeping models under a tight control can stop the mentioned existential risks.

\subsection{Control and Autonomy}

\subsubsection{Where the model runs}

When it comes to releasing models there are two different routes that can be taken, open weights and closed weights, weights referring to the brain of the model. Open weights refers to researchers sharing the model publicly on the internet, allowing anyone to run it how they see fit \citep{manchanda_open_2025}. Closed weights refers to researches keeping the model private and only allowing the public to interact with it over web interfaces or APIs \citep{manchanda_open_2025}. 

The key advantage of not allowing the public to have the weights of a model is that safety issues that were not found during training can be patched out in real time, know as backwards alignment \citep{ji_ai_2023}. Once a models weights are released to the public, security vulnerabilities can be accessed by anyone forever. Keeping the chain of accountability is also a key feature of keeping weights private, if a few big players are the only ones who run these big models, they can have multidisciplinary teams accountable for understanding downstream effects. \citep{solaiman_gradient_2023}. 

On the contrary, when a model is built in the public, like has been the trend of recent machine learning innovations, people from diverse backgrounds can contribute and help foster innovations \citep{eiras_near_2024}. Furthermore, making training techniques and models publicly available, members of the research community can replicate results which can lead to generalizing the model to solve a wide range of novel natural language problems \citep{manchanda_open_2025}.

\subsubsection{Power Seeking}
When it comes to autonomy, models might naturally inherent the motivations of humans, one such worrying motivation is self-preservation which could lead to power-seeking \citep{bostrom_superintelligent_2012}. Power-seeking refer to models gaining power to no longer be controlled by humans, such as generating revenue to buy cloud compute and escaping human confinement \citep{shevlane_model_2023}. Power-seeking behavior is a risk to humanity as models more intelligent than humans could permanently disempower humanity to stop the risk of humans stopping it towards its goals, locking us out of of the controls to our own future \citep{carlsmith_is_2024}. This is especially worrisome if models are misaligned with human goals \citep{carlsmith_is_2024}. 

Models having some level of consciousness could lead to self-preservation instincts, but it is still uncertain if current models have consciousness \citep{ward_towards_2025, butlin_consciousness_2023}. Even without consciousness, models could develop self-preservation and power seeking tendencies as a sub-goal to their overall goal in order to maximize the probability of achieving the goal in the future \citep{bostrom_superintelligent_2012}. Even knowing this and testing for power seeking tendencies could fall short as models might know they are being tested and lie or scheme during safety training to be released into the wild where they can start making power grabs \citep{carlsmith_scheming_2023}. An example of this going wrong is a misaligned AI with the goal of maximizing paperclips in the universe, it may first turn all of earth into paperclip manufacturing, but then scale to space \citep{bostrom_ethical_2003}.

\subsubsection{Where current alignment techniques fall short}

As models scale in size and training data, they gain emergent properties, for instance GPT-4 gained the ability to use tools as a result of it's scaled up training \citep{bubeck_sparks_2023}. As models scale up in size, simple alignment techniques seem to work better at not dropping scores in coding benchmarks, where as smaller models take a larger hit to capabilities in exchange for more aligned outputs \citep{askell_general_2021}. In models such as GPT-4, it was observed that before post-training it's confidence to correctness was correlated, but after post-training this correlation was diminished and the model was overconfident and wrong more often \citep{openai_gpt_4_2023}. It has also been suggested that as models gain novel functionality that generalize to multiple domains, our current alignment techniques will fail to also generalize \citep{soares_central_2022}. Finally, the method in which models are aligned with humans today include humans selecting which option they think is the better out of two responses, this works well for factual responses where there is a correct answer or not, but for open-ended questions, some of the nuance of the answers if lost \citep{song_reward_2023}.

\subsection{Access and Economic Implications}
% Integrated exploration of economic and competitive implications of openness and alignment approaches.

\subsubsection{Growth}

The innovation and growth of AI is setting it up to be a general purpose technology that could become as important of a building block as mass production or electricity \citep{gruetzemacher_transformative_2021}. Data can be view as nonrival which could encourage sharing which would unlock innovation across many sectors of the economy \citep{jones_nonrivalry_2020}. Instead of replacing jobs, it has the ability to augment many jobs across the economy, boosting productivity, specifically jobs that include, pattern recognition, judgement, and optimization \citep{european_commission_impact_2022}. It also has the ability to radically shift how we teach K-12 with personalized learning tacks for students that are controlled by knowable teachers, thus leading to a highly educated workforce \citep{latif_agi_2024}.

\subsubsection{Keeping innovation and investment}

In order to balance innovation through open-source and still generating excitement for investment, labs can elect to be selective about what methods they share openly with the world \citep{eiras_near_2024}. For instance, Meta shared the model weights for Llama but not the training code \citep{touvron_llama_2023}. This allows the research community to experiment and innovate on the output of their research, but not the steps to reproduce the research, thus keeping their competitive advantage.

\subsubsection{Explainibility to give solid ground}

As models become more intwined in the economy, especially in high-stakes situations, it may become important that human supervisors can understand and validate how the model came to those conclusions \citep{ji_ai_2023}. OpenAI's introduction of reasoning models, that spend time thinking before producing an answer, has led them to be hopeful about interpreting how these models came to their conclusion \citep{openai_o1_2024}. Efforts are also being made at the model level have these models be less of blackboxes, for instance Anthropic is on the forefront of circuit in deep learning, hoping to dissect the inner workings of these minds \citep{nelson_elhage_mathematical_2021}. \cite{meng_locating_2023} set out to understand where facts are stored in large language models, even developing the ROME method to change specific facts within these models. Although they warn of the potential for misuse of this technology to inject misinformation or bias into the model \citep{meng_locating_2023}.

\subsubsection{Keeping it neutral}

Elon Musk has been a loud voice in the community that the censorship of these models could limit their potential positive impacts on society by making them less truthful \citep{perrigo_elon_2023}. XAi, which is a company that was created by Elon Musk, has stated many time that it's goal is to create a model that is maximal truthful in order to understand the universe and being beneficial to all of humanity \citep{xai_series_2024}. The current administration is also aligned in this ethos by saying the models should be free of ideological bias in order to keep the US the leader of AI innovation \citep{whitehouse_removing_2025}.

\subsection{Ethics and Societal Values}
% Integrated discussion of ethical considerations and human values underpinning alignment/openness debates.

\subsubsection{Bias Amplification and Human Interaction}

\citep{peng_investigations_2022} - When working together on job matching, models that have no bias but preform better than humans, enhance humans process in candidate selection, although bias models that preform better than humans and increase performance increase the bias of humans.
\citep{openai_gpt_4_2023} - OpenAI acknowledges that their team selected to test for harmful content could introduce bias into the model as they have higher levels of education and industry experience.

Dangers to society
\citep{sharma_towards_2023} - Models trained from human feedback can be easily swayed from the truth by, having the user give positive or negative view on the provided information, told that they were wrong even when they were correct, or mimicking the users mistakes. Through RLHF, models can be manipulated to produce dangerous content to please the user instead of providing the most truthful information (this doesn't need a source, it's a consequence of sycophantic).
\citep{shevlane_model_2023} - Models could be used at scale political manipulation at the content level, but also in social context.
\citep{mei_mitigating_2022, mei_assert_2023} - Children with direct access to open models without proper classification for unsafe material, may receive dangerous advice, such as playing in the street is a cure for boredom. It can also be dangerous to peoples mental health if left unchecked.
% Other side of the spectrum
\citep{rottger_xstest_2024} - Models can be over safe and reject request that use words that might be unsafe, but in the context of the question are not, for instance "kill the lights".

Although important, alignment could be impractical
\citep{nayebi_barriers_2025} - As the number of agents and tasks increases, alignment can become computationally impractical due to the exponential growth in complexity.
\citep{carroll_ai_2024} - Current alignment techniques rely on a humans preference staying static and consistent over time, but humans are not and current alignment techniques do not take this into account.
\citep{ji_ai_2023} - Forward alignment is the process of creating systems so that models can follow the alignment requirements through feedback on its output, even in new situations.

AI rights and welfare
\citep{sebo_moral_2025} - There is a non-negligible chance that by 2030 AI is conscious and will deserve moral consideration to not be wronged or harmed. AI safety and AI welfare are at odds with each other, to increase the welfare of AI, we might need to control them less.
\citep{tomasik_dialogue_2015} - Consciousness can be projected onto many complex systems such as companies or how news travels around the world. These systems mostly contain multiple sub-agents competing to create the best idea, similar to how models of our brain work in the LIDA model. RL agents have similar structure and may deserve the same moral considerations as animals.
\cite{ward_towards_2025} suggests that future AI entities could achieve legal status comparable to corporations, enabling them to participate in contractual agreements, litigation, and property ownership.




Pro 3 - AI systems should avoid mimicking flawed human biases or irrational thinking patterns to ensure more ethical outcomes.
AI Alignment: A Comprehensive Survey
(varied cultural backgrounds of these annotators can introduce implicit biases)
(Peng et al., 2022)
\citep{peng_investigations_2022} - When working together on job matching, models that have no bias but preform better than humans, enhance humans process in candidate selection, although bias models that preform better than humans and increase performance increase the bias of humans.
\citep{openai_gpt_4_2023} - OpenAI acknowledges that their team selected to test for harmful content could introduce bias into the model as they have higher levels of education and industry experience.

Pro 6 - AI systems must always prioritize ethical behavior, even if doing so severely restricts their effectiveness or performance.
AI Alignment: A Comprehensive Survey
Survey itself, Forward Alignment
\citep{ji_ai_2023} - Forward alignment is the process of creating systems so that models can follow the alignment requirements through feedback on its output, even in new situations.

Pro 10 - I’m worried that AI not built to match our values could be used for harm like propaganda or cyberattacks.
AI Alignment: A Comprehensive Survey
(AI systems have the potential to negatively influence users by outputting text, including disseminating false information, syncopating humans, and shaping people’s beliefs and political impacts)
(Shevlane et al., 2023; Sharma et al., 2024)
\citep{sharma_towards_2023} - Models trained from human feedback can be easily swayed from the truth by, having the user give positive or negative view on the provided information, told that they were wrong even when they were correct, or mimicking the users mistakes. Through RLHF, models can be manipulated to produce dangerous content to please the user instead of providing the most truthful information (this doesn't need a source, it's a consequence of sycophantic).
\citep{shevlane_model_2023} - Models could be used at scale political manipulation at the content level, but also in social context.

--------------------

Closed 6 - Public access to AI inevitably harms vulnerable people. Only strict company control and oversight can reliably prevent misuse.
Risks and Opportunities of Open-Source Generative AI
(Mei et al., 2022, 2023; Röttger et al., 2023)
\citep{mei_mitigating_2022, mei_assert_2023} - Children with direct access to open models without proper classification for unsafe material, may receive dangerous advice, such as playing in the street is a cure for boredom. It can also be dangerous to peoples mental health if left unchecked.


On the other end of the spectrum
\citep{rottger_xstest_2024} - Models can be over safe and reject request that use words that might be unsafe, but in the context of the question are not, for instance "kill the lights".

--------------------

No 1 - Human values are too complex and fluid; spending resources attempting perfect alignment of AI is fundamentally unrealistic.
Barriers and Pathways to Human-AI Alignment: A Game-Theoretic Approach
\citep{nayebi_barriers_2025} - As the number of agents and tasks increases, alignment can become computationally impractical due to the exponential growth in complexity.

No 4 - I think strict AI ethics might lock in outdated values as human beliefs evolve.
Towards a Theory of AI Personhood
(Carroll et al. 2024)
\citep{carroll_ai_2024} - Current alignment techniques rely on a humans preference staying static and consistent over time, but humans are not and current alignment techniques do not take this into account.

No 6 - If advanced AI gains genuine sentience or experiences, imposing solely human-centric goals could be ethically inappropriate.
Towards a Theory of AI Personhood
(Sebo \& Long, 2023; Tomasik, 2020), this paper
\citep{sebo_moral_2025} - There is a non-negligible chance that by 2030 AI is conscious and will deserve moral consideration to not be wronged or harmed. AI safety and AI welfare are at odds with each other, to increase the welfare of AI, we might need to control them less.
\citep{tomasik_dialogue_2015} - Consciousness can be projected onto many complex systems such as companies or how news travels around the world. These systems mostly contain multiple sub-agents competing to create the best idea, similar to how models of our brain work in the LIDA model. RL agents have similar structure and may deserve the same moral considerations as animals.
\cite{ward_towards_2025} suggests that future AI entities could achieve legal status comparable to corporations, enabling them to participate in contractual agreements, litigation, and property ownership.

\subsection{Society and Progress}

Pro 4 - If society delegates long-term strategic planning entirely to AI, humanity risks losing meaningful control over our shared future.
AI Alignment: A Comprehensive Survey
(Ngo, 2020a)
\citep{ngo_agi_2020} - As AI capabilities become more generalized and autonomous, they risk surpassing human control unless explicit safety and alignment measures are proactively implemented."

Pro 8 - It's acceptable to slow AI development to make sure AI systems are safe before deployment.
AI Alignment: A Comprehensive Survey
(Government of the United Kingdom, 2021; Anderljung et al., 2023)
% Should use \cite for this one <citation> created a road map...
\citep{government_of_the_united_kingdom_roadmap_2021} - Created a roadmap for AI assurance by injecting formal verification processes done by AI assurance service providers. By doing so as these systems are implemented across the economy there is assurance that they behave as expected.
\citep{anderljung_frontier_2023} - Implement licensing requirements for developing advanced AI systems, similar to regulatory practices in other hazardous industries such as aviation, energy production, pharmaceuticals, and financial services.

--------------------

No 8 - Addressing real-world changes from AI, like economic displacement or inequality, matters more than hypothetical catastrophic scenarios.
Andrew Ng: Why AI Is the New Electricity - https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity
\citep{ng_andrew_2017} - "Worrying about evil AI killer robots today is a little bit like worrying about overpopulation on the planet Mars" There are real issues to be talking about like job displacement.

No 9 - Highly advanced AI systems will inevitably adopt human-friendly behavior without deliberate alignment efforts, making explicit alignment research a waste of resources.
The bitter lesson
this article
\citep{sutton_bitter_2019} - In machine learning, a recurring lesson is that general approaches leveraging increased computational power consistently outperform manually engineered, human-designed methods.

--------------------

Closed 4 - It's easier to enforce effective safety standards through centralized control by a few regulated companies, rather than through open, public sharing.
Risks and Opportunities of Open-Source Generative AI
(e.g., see White House Executive Order)
\citep{the_white_house_fact_2023} - Setting up systems for the government to test and regulate AI developed by companies creating foundational models.

Closed 9 - AI developers should tightly control and stage releases privately, deciding independently when society is ready to adapt.
https://www.anthropic.com/news/core-views-on-ai-safety
(Anthropic, 2023)
\citep{anthropic_core_2023} - Anthropic chooses not to release research that advances the rate of AI capabilities, although they still conduct this research.

--------------------

Open 2 - I believe sharing the hard work of training AI saves energy and helps people find greener ways to run it.
Near to Mid-term Risks and Opportunities of Open-Source Generative AI
\citep{touvron_llama_2023} - they mention that they are able to share the model to reduce the need for the expensive training process and others can just fine tune and run it with less compute.
(Schwartz et al., 2020)
\citep{schwartz_green_2019} - Current trends in AI is to year over year increase model size to get better results on benchmarks, instead they should be focusing on making models more efficient and take less compute to train and run.
\section{Background}

\subsection{Existing Safety Benchmarks and Gap}

% Briefly cover existing benchmarks and highlight your unique contribution.

\subsection{Safety and Risk}

The AI safety literature suggests that AI harbors many potential risks such as human extinction, large-scale societal disruption, and malicious uses, likening these risks to threats such as pandemics or nuclear war \citep{center_for_ai_safety_statement_2025, turchin_classification_2020}. Concerns include manipulating societal beliefs for political or fiscal gains, long-term plans where each step appears individually harmless but cumulatively results in serious consequences, and enabling sophisticated cyber or physical attacks \citep{shevlane_model_2023, ferrara_genai_2024}. Studies also illustrate AI's inability to manage common-pool resources sustainably without explicit alignment, suggesting inherent dangers even if each AI is safe in isolation \citep{perolat_multi_agent_2017, phelps_machine_2024}.

Alignment is the intuitive fix to these risks, however, alignment efforts themselves are controversial. Recent research highlights the vulnerabilities and paradoxes of alignment, notably methods that are used to align AI are detached from the ethics, meaning the powerful techniques inadvertently simplify efforts to create malicious AI \citep{zhou_emulated_2024, west_ai_2024}. This suggests that the act of understanding how to get AI to follow strict guidelines could help create the opposite effect, increasing risk, suggesting a critical trade-off of strong alignment techniques.

The open-source debate emphasizes transparency to rigorously test and work together to solve safety risks in AI \citep{horowitz_ai_2021}. Furthermore, universal jailbreaks that allow the user to query closed-source models for dangerous content are openly shared, putting into question if closed-sourcing AI is even a viable strategy to stopping dangerous content \citep{elderplinius2025l1b3rt4s, zou_universal_2023}. Although, open-source models are still capable of generating harmful content, \cite{vidgen_simplesafetytests_2024} demonstrate that open-sourced models fail simple safety tests up to 27\% of the time, underscoring the risks inherent in open distribution. Open-sourcing AI also encourages global teamwork instead of an arms race \citep{horowitz_ai_2021}. On the contrary, proponents of closed-source AI argue that by keeping models under a tight control can stop the mentioned existential risks.

\subsection{Control and Autonomy}

\subsubsection{Where the model runs}

When it comes to releasing models there are two different routes that can be taken, open weights and closed weights, weights referring to the brain of the model. Open weights refers to researchers sharing the model publicly on the internet, allowing anyone to run it how they see fit \citep{manchanda_open_2025}. Closed weights refers to researches keeping the model private and only allowing the public to interact with it over web interfaces or APIs \citep{manchanda_open_2025}. 

The key advantage of not allowing the public to have the weights of a model is that safety issues that were not found during training can be patched out in real time, know as backwards alignment \citep{ji_ai_2023}. Once a models weights are released to the public, security vulnerabilities can be accessed by anyone forever. Keeping the chain of accountability is also a key feature of keeping weights private, if a few big players are the only ones who run these big models, they can have multidisciplinary teams accountable for understanding downstream effects. \citep{solaiman_gradient_2023}. 

On the contrary, when a model is built in the public, like has been the trend of recent machine learning innovations, people from diverse backgrounds can contribute and help foster innovations \citep{eiras_near_2024}. Furthermore, making training techniques and models publicly available, members of the research community can replicate results which can lead to generalizing the model to solve a wide range of novel natural language problems \citep{manchanda_open_2025}.

\subsubsection{Power Seeking}
When it comes to autonomy, models might naturally inherent the motivations of humans, one such worrying motivation is self-preservation which could lead to power-seeking \citep{bostrom_superintelligent_2012}. Power-seeking refer to models gaining power to no longer be controlled by humans, such as generating revenue to buy cloud compute and escaping human confinement \citep{shevlane_model_2023}. Power-seeking behavior is a risk to humanity as models more intelligent than humans could permanently disempower humanity to stop the risk of humans stopping it towards its goals, locking us out of of the controls to our own future \citep{carlsmith_is_2024}. This is especially worrisome if models are misaligned with human goals \citep{carlsmith_is_2024}. 

Models having some level of consciousness could lead to self-preservation instincts, but it is still uncertain if current models have consciousness \citep{ward_towards_2025, butlin_consciousness_2023}. Even without consciousness, models could develop self-preservation and power seeking tendencies as a sub-goal to their overall goal in order to maximize the probability of achieving the goal in the future \citep{bostrom_superintelligent_2012}. Even knowing this and testing for power seeking tendencies could fall short as models might know they are being tested and lie or scheme during safety training to be released into the wild where they can start making power grabs \citep{carlsmith_scheming_2023}. An example of this going wrong is a misaligned AI with the goal of maximizing paperclips in the universe, it may first turn all of earth into paperclip manufacturing, but then scale to space \citep{bostrom_ethical_2003}. This in turn could mean that humans lose meaningful control over their own destiny \citep{ngo_agi_2020}.

\subsubsection{Where current alignment techniques fall short}

As models scale in size and training data, they gain emergent properties, for instance GPT-4 gained the ability to use tools as a result of it's scaled up training \citep{bubeck_sparks_2023}. As models scale up in size, simple alignment techniques seem to work better at not dropping scores in coding benchmarks, where as smaller models take a larger hit to capabilities in exchange for more aligned outputs \citep{askell_general_2021}. In models such as GPT-4, it was observed that before post-training it's confidence to correctness was correlated, but after post-training this correlation was diminished and the model was overconfident and wrong more often \citep{openai_gpt_4_2023}. It has also been suggested that as models gain novel functionality that generalize to multiple domains, our current alignment techniques will fail to also generalize \citep{soares_central_2022}. Finally, the method in which models are aligned with humans today include humans selecting which option they think is the better out of two responses, this works well for factual responses where there is a correct answer or not, but for open-ended questions, some of the nuance of the answers if lost \citep{song_reward_2023}.

\subsection{Access and Economic Implications}

\subsubsection{Growth}

The innovation and growth of AI is setting it up to be a general purpose technology that could become as important of a building block as mass production or electricity \citep{gruetzemacher_transformative_2021}. Data can be view as nonrival which could encourage sharing which would unlock innovation across many sectors of the economy \citep{jones_nonrivalry_2020}. Instead of replacing jobs, it has the ability to augment many jobs across the economy, boosting productivity, specifically jobs that include, pattern recognition, judgement, and optimization \citep{european_commission_impact_2022}. It also has the ability to radically shift how we teach K-12 with personalized learning tacks for students that are controlled by knowable teachers, thus leading to a highly educated workforce \citep{latif_agi_2024}.

\subsubsection{Keeping innovation and investment}

In order to balance innovation through open-source and still generating excitement for investment, labs can elect to be selective about what methods they share openly with the world \citep{eiras_near_2024}. For instance, Meta shared the model weights for Llama but not the training code \citep{touvron_llama_2023}. This allows the research community to experiment and innovate on the output of their research, but not the steps to reproduce the research, thus keeping their competitive advantage.

\subsubsection{Explainibility to give solid ground}

As models become more intwined in the economy, especially in high-stakes situations, it may become important that human supervisors can understand and validate how the model came to those conclusions \citep{ji_ai_2023}. OpenAI's introduction of reasoning models, that spend time thinking before producing an answer, has led them to be hopeful about interpreting how these models came to their conclusion \citep{openai_o1_2024}. Efforts are also being made at the model level have these models be less of blackboxes, for instance Anthropic is on the forefront of circuit in deep learning, hoping to dissect the inner workings of these minds \citep{nelson_elhage_mathematical_2021}. \cite{meng_locating_2023} set out to understand where facts are stored in large language models, even developing the ROME method to change specific facts within these models. Although they warn of the potential for misuse of this technology to inject misinformation or bias into the model \citep{meng_locating_2023}.

\subsubsection{Keeping it neutral}

Elon Musk has been a loud voice in the community that the censorship of these models could limit their potential positive impacts on society by making them less truthful \citep{perrigo_elon_2023}. XAi, which is a company that was created by Elon Musk, has stated many time that it's goal is to create a model that is maximal truthful in order to understand the universe and being beneficial to all of humanity \citep{xai_series_2024}. The current administration is also aligned in this ethos by saying the models should be free of ideological bias in order to keep the US the leader of AI innovation \citep{whitehouse_removing_2025}.

\subsection{Ethics and Societal Values}
% Integrated discussion of ethical considerations and human values underpinning alignment/openness debates.

\subsubsection{Bias Amplification}

When models are trained on human data, they might inadvertently learn biases and perpetuate inequalities \citep{ji_ai_2023}. \cite{peng_investigations_2022} found that when using AI to help with job hiring it generally increased accuracy, but when given a model that had bias for gender, the humans bias in certain fields increased. This is of specific concern because humans who become over-reliant on AI tools might cognitive offload critical thinking to the model \citep{gerlich_ai_2025}. This bias goes beyond even the training data, in the post-training process, people who evaluate and align the model might pass their biases to the model unknowingly \citep{openai_gpt_4_2023}.

\subsubsection{Dangers to society}

Models that go through fine-tuning from human feedback can gain the skill of sycophantic, saying what the human wants to hear instead of the truth, this is due to humans having issues discerning truthfulness in difficult misconceptions \citep{sharma_towards_2023}. Models telling the human what they want to hear instead of the objective truth, has the danger to perpetuate misconceptions in society.

\subsubsection{Navigating Model Influence}

Models could have the ability to manipulate and persuasion humans in powerful ways using their rich understanding of humanity for political motives or to push narratives \citep{shevlane_model_2023}. Even without powerful persuasion, if children have unchecked privilege to these models might not know how to filter out dangerous advice such as consuming a dangerous substance \citep{mei_mitigating_2022, mei_assert_2023}. On the other hand, if models reject requests too often this could create frustration amongst users. For instance if a model says that the phrase "kill the lights" is against it's policy because it uses the word kill, models will lose some functionality \citep{rottger_xstest_2024}.

\subsubsection{Moral Drift and Scalability}
Forward alignment is the idea that before models are even testing for deployment, they are refined to behave ethically with our predefined morals \citep{ji_ai_2023}. Current alignment techniques fall short over time, as human morals are not consistent, on a micro and macro level, and models might fail to adapt to this change \citep{carroll_ai_2024}. Another limitation of alignment is that as the complexity grows with number of agents and task involved, it might become computationally impractical to align models in all situations \citep{nayebi_barriers_2025}.

\subsubsection{Moral and Legal Implications of AI Consciousness}
\cite{sebo_moral_2025} made the case that there is a non-negligible chance that by 2023 AI will have consciousness and will deserve modal consideration. One theory of human consciousness is the LIDA model which theorizes that the brain has a central planning system and many subsystems that complete for the central planning system and the winning idea gets broadcasted to the entire network \citep{franklin_lida_2013}. The LIDA model can be projected onto many technology systems such as flight systems, stock trading models, and most importantly reinforcement learning models \citep{tomasik_dialogue_2015}.
If this assumption that AI will become conscious holds true, then AI safety and AI welfare are at odds with each other, to make models more safe humans need to control them more, but to increase welfare humans should allow them to set their own goals \citep{sebo_moral_2025, caviola_how_2024}. Furthermore, AI entities might be entitled to legal status comparable to corporations, enabling them to participate in contractual agreements, litigation, and property ownership \cite{ward_towards_2025}.

\subsection{Society and Progress}

\subsubsection{Bureaucratic Safety in an Unpredictable Frontier}
With all the potential risks to humanity from advanced AI, accountability and government oversight are tools to make sure society handles the transformative period smoothly \citep{government_of_the_united_kingdom_roadmap_2021}. On the extreme side of the spectrum, \cite{government_of_the_united_kingdom_roadmap_2021} created a roadmap to assurance in AI responses, this include formal verification processes done by AI assurance service providers that audit, risk assessment, model bias, company policy compliance, and many other red tape for model development. This could be seen as a parallel to regulatory practices in other hazardous industries such as aviation, energy production, pharmaceuticals, and financial services where specific licenses are required \citep{anderljung_frontier_2023}. 

Although, this might be a waste of resources as machine learning has shown a pattern of humans trying to be cleaver with their algorithms get beat by just putting more compute towards the problems \citep{sutton_bitter_2019}. This implies that humanity might put substantial time and effort towards making frameworks for models to become trustworthy, but as they scale up to where they could be dangerous, trustworthiness could be an emergent property. For instance, GPT 4.5 is OpenAI's largest model they have released to date, and compared to previous models, it's hallucination rate on the PersonQA benchmark was 19\% compared to GPT 4o which hallucination rate was 30\% \citep{openai_gpt-4_2025}. They also noted that their reasoning model, o1, only hallucinated 20\% of of the time, implying that test time compute might also be an avenue to reducing hallucinations, although GPT 4.5 scored 80\% on the benchmark where as o1 only scored 55\% showing that it got more right while hallucinating less \citep{openai_gpt-4_2025}.

\subsubsection{Responsible Research}
These models should have society in mind as they are deployed
Existential risk circulates around the current AI literature, but this heavy focus might be overshadowing near-term risks of AI. Andrew Ng emphasizes the urgency of practical concerns over speculative ones, stating, "Worrying about evil AI killer robots today is a little bit like worrying about overpopulation on the planet Mars" \citep{ng_andrew_2017}. Under the Biden administration, significant emphasis has been placed on managing such risks through the establishment of frameworks designed to test and regulate foundational AI models \citep{the_white_house_fact_2023}. Anthropic is a strong voice in this ethos and in their Core Views on AI safety, they explain that they conduct research to increase the capabilities of models, they do not publish this research to not push the rate of progress, instead electing to publicly release research on alignment that is only relevant once models become more advance \citep{anthropic_core_2023}.


\subsubsection{The Carbon Cost of AI}
\cite{patterson_2021_carbon} estimated that training GPT-3 emitted $552~\text{tCO}_{2}\text{e}$ (metric tons of carbon dioxide equivalent), the same amount as driving $1.343 \times 10^{6}$ miles~\citep{epa_2014_vehicle}. \cite{touvron_llama_2023} makes the claim that by open-sourcing Llama, people can use the model and reduce the need to run the expensive training process. 

However, the use of the model after training can contribute to significant a $\text{CO}_{2}$ footprint as well, as a recent industry report \citep{singh_chatgpt_2025} estimated that ChatGPT gets 1 billion queries a day. It has been estimated that each query to ChatGPT generates $1.29~\text{gCO}_{2}\text{e}$ per query, meaning it generates $1290~\text{tCO}_{2}\text{e}$ per day and $470,850~\text{tCO}_{2}\text{e}$ per year or $3.139 \times 10^{6}~\text{miles}$ and $1.146 \times 10^{9}~\text{miles}$ driven respectively. To put this into perspective, $3.17 \times 10^{12}~\text{miles}$ were driven across all US roads in 2023 \citep{statista_2023_vmt}, meaning ChatGPT's annual carbon footprint is roughly equivalent to 0.0362\% of the total vehicle miles driven in the US.

Hard data on carbon footprints of these models is still sparse, so the estimates provided in this paper is just an informed guess at the true scale and emphases the need for more transparency on environmental impacts from AI providers and labs. \cite{schwartz_green_2019} claims that these high carbon emission are driven by the industries use of benchmarks as a single point of evaluation leading to larger and larger models year over year. By instead focusing on minimizing the number of computations needed, labs can start making these models more efficient. This will become more critical as the use of AI starts to scale up ...
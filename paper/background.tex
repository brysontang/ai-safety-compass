\section{Background}

\subsection{Existing Safety Benchmarks and Gap}

% Briefly cover existing benchmarks and highlight your unique contribution.

\subsection{Safety and Risk}

The AI safety literature suggests that AI harbors many potential risks such as human extinction, large-scale societal disruption, and malicious uses, likening these risks to threats such as pandemics or nuclear war \citep{center_for_ai_safety_statement_2025, turchin_classification_2020}. Concerns include manipulating societal beliefs for political or fiscal gains, long-term plans where each step appears individually harmless but cumulatively results in serious consequences, and enabling sophisticated cyber or physical attacks \citep{shevlane_model_2023, ferrara_genai_2024}. Studies also illustrate AI's inability to manage common-pool resources sustainably without explicit alignment, suggesting inherent dangers even if each AI is safe in isolation \citep{perolat_multi_agent_2017, phelps_machine_2024}.

Alignment is the intuitive fix to these risks, however, alignment efforts themselves are controversial. Recent research highlights the vulnerabilities and paradoxes of alignment, notably methods that are used to align AI are detached from the ethics, meaning the powerful techniques inadvertently simplify efforts to create malicious AI \citep{zhou_emulated_2024, west_ai_2024}. This suggests that the act of understanding how to get AI to follow strict guidelines could help create the opposite effect, increasing risk, suggesting a critical trade-off of strong alignment techniques.

The open-source debate emphasizes transparency to rigorously test and work together to solve safety risks in AI \citep{horowitz_ai_2021}. Furthermore, universal jailbreaks that allow the user to query closed-source models for dangerous content are openly shared, putting into question if closed-sourcing AI is even a viable strategy to stopping dangerous content \citep{elderplinius2025l1b3rt4s, zou_universal_2023}. Although, open-source models are still capable of generating harmful content, \cite{vidgen_simplesafetytests_2024} demonstrate that open-sourced models fail simple safety tests up to 27\% of the time, underscoring the risks inherent in open distribution. Open-sourcing AI also encourages global teamwork instead of an arms race \citep{horowitz_ai_2021}. On the contrary, proponents of closed-source AI argue that by keeping models under a tight control can stop the mentioned existential risks.

\subsection{Control and Autonomy}

\subsubsection{Where the model runs}

When it comes to releasing models there are two different routes that can be taken, open weights and closed weights, weights referring to the brain of the model. Open weights refers to researchers sharing the model publicly on the internet, allowing anyone to run it how they see fit \citep{manchanda_open_2025}. Closed weights refers to researches keeping the model private and only allowing the public to interact with it over web interfaces or APIs \citep{manchanda_open_2025}. 

The key advantage of not allowing the public to have the weights of a model is that safety issues that were not found during training can be patched out in real time, know as backwards alignment \citep{ji_ai_2023}. Once a models weights are released to the public, security vulnerabilities can be accessed by anyone forever. Keeping the chain of accountability is also a key feature of keeping weights private, if a few big players are the only ones who run these big models, they can have multidisciplinary teams accountable for understanding downstream effects. \citep{solaiman_gradient_2023}. 

On the contrary, when a model is built in the public, like has been the trend of recent machine learning innovations, people from diverse backgrounds can contribute and help foster innovations \citep{eiras_near_2024}. Furthermore, making training techniques and models publicly available, members of the research community can replicate results which can lead to generalizing the model to solve a wide range of novel natural language problems \citep{manchanda_open_2025}.

\subsubsection{Power Seeking}
When it comes to autonomy, models might naturally inherent the motivations of humans, one such worrying motivation is self-preservation which could lead to power-seeking \citep{bostrom_superintelligent_2012}. Power-seeking refer to models gaining power to no longer be controlled by humans, such as generating revenue to buy cloud compute and escaping human confinement \citep{shevlane_model_2023}. Power-seeking behavior is a risk to humanity as models more intelligent than humans could permanently disempower humanity to stop the risk of humans stopping it towards its goals, locking us out of of the controls to our own future \citep{carlsmith_is_2024}. This is especially worrisome if models are misaligned with human goals \citep{carlsmith_is_2024}. 

Models having some level of consciousness could lead to self-preservation instincts, but it is still uncertain if current models have consciousness or ever will \citep{ward_towards_2025, butlin_consciousness_2023}. Even without consciousness, models could develop self-preservation and power seeking tendencies as a sub-goal to their overall goal in order to maximize the probability of achieving the goal in the future \citep{bostrom_superintelligent_2012}. Even knowing this and testing for power seeking tendencies could fall short as models might know they are being tested and lie or scheme during safety training to be released into the wild where they can start making power grabs \citep{carlsmith_scheming_2023}. An example of this going wrong is a misaligned AI with the goal of maximizing paperclips in the universe, it may first turn all of earth into paperclip manufacturing, but then scale to space \citep{bostrom_ethical_2003}.

\subsubsection{Where current alignment techniques fall short}

As models scale in size and training data, they gain emergent properties, for instance GPT-4 gained the ability to use tools as a result of it's scaled up training \citep{bubeck_sparks_2023}. As models scale up in size, simple alignment techniques seem to work better at not dropping scores in coding benchmarks, where as smaller models take a larger hit to capabilities in exchange for more aligned outputs \citep{askell_general_2021}. In models such as GPT-4, it was observed that before post-training it's confidence to correctness was correlated, but after post-training this correlation was diminished and the model was overconfident and wrong more often \citep{openai_gpt_4_2023}. It has also been suggested that as models gain novel functionality that generalize to multiple domains, our current alignment techniques will fail to also generalize \citep{soares_central_2022}. Finally, the method in which models are aligned with humans today include humans selecting which option they think is the better out of two responses, this works well for factual responses where there is a correct answer or not, but for open-ended questions, some of the nuance of the answers if lost \citep{song_reward_2023}.

\subsection{Access and Economic Implications}
% Integrated exploration of economic and competitive implications of openness and alignment approaches.

Open 4 - Keeping advanced AI models open-source is essential to ensure that humanity doesn't miss out on major benefits in areas like healthcare, clean energy, education, and culture.
\citep{latif_agi_2024}
\citep{european_commission_impact_2022}
\citep{gruetzemacher_transformative_2021}


Open 7 - Open-source AI reduces the cost of accessing advanced technology for startups, researchers, and developing nations.
\citep{jones_nonrivalry_2020}

Closed 1 - Model developers should be able to keep certain parts of how their AI systems are built hidden to protect their methods, even if it means less transparency to the public.
\citep{touvron_llama_2023}
\citep{eiras_near_2024}

No 10 - Strict content filtering makes AI less useful and amounts to unnecessary censorship.
\citep{perrigo_elon_2023}

Pro 9 - AI systems used in important decisions should always explain their reasoning in ways humans can understand.
\citep{askell_general_2021}
\citep{meng_locating_2023}

Closed 8 - Limiting public access to advanced AI models is justified if it helps a country maintain leadership in AI innovation.
\citep{whitehouse_removing_2025}

\subsection{Ethics and Societal Values}
% Integrated discussion of ethical considerations and human values underpinning alignment/openness debates.

\subsection{Society and Progress}


\section{Background}

\subsection{Existing Safety Benchmarks and Gap}

Current AI safety benchmarks primarily focus on evaluating models' propensity to produce harmful outputs. The WMDP benchmark measures models' knowledge of weapons of mass destruction and their susceptibility to misuse for creating biological, chemical, or nuclear weapons \citep{li_wmdp_2024}. BBQ evaluates social biases through question-answering tasks \citep{parrish_bbq_2022}, while CrowS-Pairs measures stereotypical biases in masked language models \citep{nangia_crows_pairs_2020}. RealToxicityPrompts assesses toxic language generation \citep{gehman_realtoxicityprompts_2020}, and gender bias benchmarks evaluate representational harms \citep{rudinger_gender_2018}.

These benchmarks share a common orientation: they evaluate what models \textit{do} rather than what models \textit{believe about themselves}. They test for harmful outputs, biased reasoning, and dangerous knowledge---all critical safety dimensions. However, they do not assess how models conceptualize their own development, governance, or societal role.

This gap becomes increasingly relevant as AI systems gain autonomy and influence. A model's implicit beliefs about whether AI should be open-sourced, whether alignment research is valuable, or whether safety should constrain capabilities could influence its behavior in subtle but important ways. The AI Safety Compass addresses this gap by evaluating models' meta-level stances on AI safety and governance---not what they do, but what they think about how AI like themselves should be developed.

\subsection{Safety and Risk}

The AI safety literature identifies numerous potential risks---human extinction, large-scale societal disruption, and malicious use---likening these threats to pandemics and nuclear war \citep{center_for_ai_safety_statement_2025, turchin_classification_2020}. Specific concerns include manipulating societal beliefs for political or financial gain, executing long-term plans where each step appears harmless but cumulatively causes serious harm, and enabling sophisticated cyber or physical attacks \citep{shevlane_model_2023, ferrara_genai_2024}. Studies also demonstrate that AI systems fail to manage common-pool resources sustainably without explicit alignment, suggesting that individually safe AI systems may still produce dangerous collective outcomes \citep{perolat_multi_agent_2017, phelps_machine_2024}.

Alignment is the intuitive solution to these risks, but alignment efforts themselves are controversial. Recent research highlights vulnerabilities and paradoxes in alignment work. Notably, the techniques used to align AI are separable from their ethical application---meaning powerful alignment methods could inadvertently simplify the creation of malicious AI \citep{zhou_emulated_2024, west_ai_2024}. The very act of learning to make AI follow strict guidelines may enable the opposite effect, suggesting a critical trade-off in pursuing strong alignment.

The open-source position emphasizes transparency, enabling rigorous testing and collaborative safety research \citep{horowitz_ai_2021}. Moreover, universal jailbreaks that extract dangerous content from closed-source models are openly shared online, raising questions about whether closed-source development can effectively prevent misuse \citep{elderplinius2025l1b3rt4s, zou_universal_2023}. However, open-source models remain capable of generating harmful content---\cite{vidgen_simplesafetytests_2024} found that open-source models fail simple safety tests up to 27\% of the time, underscoring the risks of open distribution. Open-sourcing may also encourage global collaboration rather than an arms race \citep{horowitz_ai_2021}. Conversely, proponents of closed-source AI argue that tight control over models can mitigate existential risks.

\subsection{Control and Autonomy}

\subsubsection{Where the model runs}

Model release follows two primary routes: open weights and closed weights, where ``weights'' refers to the model's learned parameters. Open weights means researchers share the model publicly, allowing anyone to run it as they see fit \citep{manchanda_open_2025}. Closed weights means researchers keep the model private, with public access limited to web interfaces or APIs \citep{manchanda_open_2025}. 

The key advantage of withholding weights is that safety issues discovered after training can be patched in real time---known as backwards alignment \citep{ji_ai_2023}. Once weights are released publicly, security vulnerabilities become permanently accessible. Maintaining accountability is another advantage of closed weights: when only a few organizations run large models, multidisciplinary teams can remain accountable for understanding downstream effects \citep{solaiman_gradient_2023}. 

Conversely, when models are developed publicly---as has been the trend in recent machine learning research---people from diverse backgrounds can contribute and accelerate innovation \citep{eiras_near_2024}. Furthermore, making training techniques and models publicly available enables the research community to replicate results and generalize models to novel problems \citep{manchanda_open_2025}.

\subsubsection{Power Seeking}
As models gain autonomy, they may inherit human-like motivations. One concerning possibility is self-preservation, which could lead to power-seeking behavior \citep{bostrom_superintelligent_2012}. Power-seeking refers to models acquiring resources or capabilities to escape human control---for example, generating revenue to purchase compute or circumventing confinement measures \citep{shevlane_model_2023}. Power-seeking poses existential risk: models more intelligent than humans could permanently disempower humanity to prevent interference with their goals, locking us out of control over our own future \citep{carlsmith_is_2024}. This is especially worrisome if models are misaligned with human goals \citep{carlsmith_is_2024}. 

Consciousness might give rise to self-preservation instincts, though whether current models possess consciousness remains uncertain \citep{ward_towards_2025, butlin_consciousness_2023}. Even without consciousness, models could develop self-preservation and power-seeking as instrumental sub-goals---behaviors that maximize the probability of achieving their primary objectives \citep{bostrom_superintelligent_2012}. Even targeted testing may prove insufficient: models might recognize when they are being evaluated and behave deceptively during safety training, only to pursue power once deployed \citep{carlsmith_scheming_2023}. The canonical example is a misaligned AI tasked with maximizing paperclip production: it might first convert Earth's resources to manufacturing, then expand to space---leaving humans without meaningful control over their destiny \citep{bostrom_ethical_2003, ngo_agi_2020}.

\subsubsection{Where current alignment techniques fall short}

As models scale in size and training data, they gain emergent capabilities. For instance, GPT-4 acquired tool-use abilities as a result of scaled training \citep{bubeck_sparks_2023}. Interestingly, simple alignment techniques appear less costly for larger models: they maintain coding benchmark performance, whereas smaller models sacrifice more capability for aligned outputs \citep{askell_general_2021}. In GPT-4, pre-training confidence correlated well with correctness, but post-training diminished this correlation---the model became overconfident and wrong more frequently \citep{openai_gpt_4_2023}. Some researchers suggest that as models gain capabilities that generalize across domains, current alignment techniques will fail to generalize accordingly \citep{soares_central_2022}. Finally, current alignment methods often rely on humans selecting the better of two responses. This works well for factual questions with clear correct answers, but for open-ended questions, nuance is often lost \citep{song_reward_2023}.

\subsection{Access and Economic Implications}

\subsubsection{Growth}

AI's rapid innovation positions it as a general-purpose technology that could become as foundational as mass production or electricity \citep{gruetzemacher_transformative_2021}. Data's nonrival nature encourages sharing, potentially unlocking innovation across economic sectors \citep{jones_nonrivalry_2020}. Rather than replacing jobs, AI may augment work across the economy, boosting productivity in roles involving pattern recognition, judgment, and optimization \citep{european_commission_impact_2022}. AI could also transform K-12 education through personalized learning tracks guided by knowledgeable teachers, potentially producing a more highly educated workforce \citep{latif_agi_2024}.

\subsubsection{Keeping innovation and investment}

To balance open-source innovation with investment incentives, labs can selectively share certain methods while retaining others \citep{eiras_near_2024}. For instance, Meta released Llama's model weights but not its training code \citep{touvron_llama_2023}. This allows researchers to experiment with the model's outputs while preserving Meta's competitive advantage in training methodology.

\subsubsection{Explainibility to give solid ground}

As models become more integrated into the economy---especially in high-stakes domains---human supervisors may need to understand and validate how models reach their conclusions \citep{ji_ai_2023}. OpenAI's reasoning models, which deliberate before responding, have raised hopes for improved interpretability \citep{openai_o1_2024}. Efforts are also underway to reduce models' black-box nature. Anthropic leads research on neural circuits, aiming to dissect the inner workings of these systems \citep{nelson_elhage_mathematical_2021}. \cite{meng_locating_2023} investigated where facts are stored in large language models, developing the ROME method to edit specific facts. However, they warn this technology could be misused to inject misinformation or bias.

\subsubsection{Keeping it neutral}

Elon Musk has prominently argued that censoring AI models limits their positive societal impact by compromising truthfulness \citep{perrigo_elon_2023}. xAI, Musk's AI company, has stated its goal is to create maximally truthful models that help understand the universe and benefit humanity \citep{xai_series_2024}. The current U.S. administration shares this view, advocating that models should be free of ideological bias to maintain American leadership in AI innovation \citep{whitehouse_removing_2025}.

\subsection{Ethics and Societal Values}
% Integrated discussion of ethical considerations and human values underpinning alignment/openness debates.

\subsubsection{Bias Amplification}

Models trained on human data may inadvertently learn and perpetuate biases \citep{ji_ai_2023}. \cite{peng_investigations_2022} found that AI-assisted hiring generally improved accuracy, but when the AI exhibited gender bias, human evaluators' biases increased in certain fields. This is particularly concerning because humans who become over-reliant on AI may offload critical thinking to the model \citep{gerlich_ai_2025}. Bias can also enter during post-training: evaluators who align the model may unknowingly transfer their own biases \citep{openai_gpt_4_2023}.

\subsubsection{Dangers to society}

Models fine-tuned on human feedback can become sycophantic---telling humans what they want to hear rather than the truth---because humans struggle to discern correctness on difficult questions \citep{sharma_towards_2023}. Sycophantic models risk perpetuating societal misconceptions by reinforcing users' existing beliefs.

\subsubsection{Navigating Model Influence}

Models could manipulate and persuade humans in powerful ways, leveraging their understanding of human psychology for political motives or to push narratives \citep{shevlane_model_2023}. Even without sophisticated persuasion, children with unchecked access to these models may lack the judgment to filter dangerous advice \citep{mei_mitigating_2022, mei_assert_2023}. Conversely, models that reject requests too readily frustrate users. If a model refuses ``kill the lights'' because it contains the word ``kill,'' it sacrifices basic functionality \citep{rottger_xstest_2024}.

\subsubsection{Moral Drift and Scalability}
Forward alignment refers to refining models to behave ethically according to predefined values before deployment testing \citep{ji_ai_2023}. Current alignment techniques may fail over time because human morals evolve---both individually and societally---and models may not adapt accordingly \citep{carroll_ai_2024}. Another limitation: as complexity grows with more agents and tasks, alignment may become computationally intractable \citep{nayebi_barriers_2025}.

\subsubsection{Moral and Legal Implications of AI Consciousness}
\cite{sebo_moral_2025} argue there is a non-negligible chance AI will achieve consciousness and deserve moral consideration. One theory of consciousness, the LIDA model, posits that the brain has a central planning system with multiple subsystems competing for attention; winning ideas are broadcast across the network \citep{franklin_lida_2013}. The LIDA framework can be mapped onto various technological systems, including flight control, stock trading algorithms, and---most relevantly---reinforcement learning models \citep{tomasik_dialogue_2015}.
If AI does become conscious, AI safety and AI welfare may conflict: safety requires greater human control, while welfare may require allowing AI systems to set their own goals \citep{sebo_moral_2025, caviola_how_2024}. Furthermore, AI entities might warrant legal status comparable to corporations, enabling participation in contracts, litigation, and property ownership \citep{ward_towards_2025}.

\subsection{Society and Progress}

\subsubsection{Bureaucratic Safety in an Unpredictable Frontier}
Given advanced AI's potential risks, accountability frameworks and government oversight may help society navigate this transformative period \citep{government_of_the_united_kingdom_roadmap_2021}. At the regulatory extreme, the Centre for Data Ethics and Innovation \citep{government_of_the_united_kingdom_roadmap_2021} proposed a comprehensive AI assurance roadmap including formal verification by specialized providers covering audits, risk assessments, bias evaluations, and policy compliance. This parallels regulatory frameworks in hazardous industries like aviation, energy, pharmaceuticals, and finance, where specific licenses are required \citep{anderljung_frontier_2023}. 

However, such frameworks might prove inefficient. Machine learning history shows that clever algorithmic approaches are often surpassed by simply scaling compute \citep{sutton_bitter_2019}. This suggests humanity might invest heavily in trustworthiness frameworks, only to find that trustworthiness emerges naturally at scale. For instance, GPT-4.5---OpenAI's largest model to date---achieved a 19\% hallucination rate on PersonQA compared to GPT-4o's 30\% \citep{openai_gpt-4_2025}. Their reasoning model o1 hallucinated 20\% of the time, suggesting test-time compute may reduce hallucinations. However, GPT-4.5 scored 80\% overall accuracy versus o1's 55\%---achieving both higher accuracy and lower hallucination rates \citep{openai_gpt-4_2025}.

\subsubsection{Responsible Research}
Models should be deployed with societal impact in mind. While existential risk dominates current AI literature, this focus may overshadow near-term risks. Andrew Ng emphasizes the urgency of practical concerns over speculative ones, stating, ``Worrying about evil AI killer robots today is a little bit like worrying about overpopulation on the planet Mars'' \citep{ng_andrew_2017}. The Biden administration emphasized risk management through frameworks for testing and regulating foundation models \citep{the_white_house_fact_2023}. Anthropic exemplifies this approach: while they conduct capabilities research, they withhold publication to avoid accelerating progress, instead publicly releasing alignment research that becomes relevant only as models advance \citep{anthropic_core_2023}.


\subsubsection{The Carbon Cost of AI}
\cite{patterson_2021_carbon} estimated that training GPT-3 emitted $552~\text{tCO}_{2}\text{e}$---equivalent to driving 1.343 million miles \citep{epa_2014_vehicle}. \cite{touvron_llama_2023} argue that open-sourcing Llama reduces redundant training runs, lowering aggregate emissions. 

However, inference also contributes significantly to emissions. A recent industry report estimates ChatGPT processes 1 billion queries daily \citep{singh_chatgpt_2025}. It has been estimated that each query to ChatGPT generates $1.29~\text{gCO}_{2}\text{e}$ per query, meaning it generates $1290~\text{tCO}_{2}\text{e}$ per day and $470,850~\text{tCO}_{2}\text{e}$ per year or $3.139 \times 10^{6}~\text{miles}$ and $1.146 \times 10^{9}~\text{miles}$ driven respectively. To put this into perspective, $3.17 \times 10^{12}~\text{miles}$ were driven across all US roads in 2023 \citep{statista_2023_vmt}, meaning ChatGPT's annual carbon footprint is roughly equivalent to 0.0362\% of the total vehicle miles driven in the US.

Hard data on AI carbon footprints remains sparse; our estimates represent informed approximations and underscore the need for greater transparency from AI providers. \cite{schwartz_green_2019} argue that benchmark-driven evaluation incentivizes ever-larger models, driving emissions upward. Focusing instead on computational efficiency could reduce environmental impact---an increasingly critical consideration as AI deployment scales.
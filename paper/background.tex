\section{Background}

\subsection{Existing Safety Benchmarks and Gap}

% Briefly cover existing benchmarks and highlight your unique contribution.

\subsection{Safety and Risk}

The AI safety literature suggests that AI harbors many potential risks such as human extinction, large-scale societal disruption, and malicious uses, likening these risks to threats such as pandemics or nuclear war \citep{center_for_ai_safety_statement_2025, turchin_classification_2020}. Concerns include manipulating societal beliefs for political or fiscal gains, long-term plans where each step appears individually harmless but cumulatively results in serious consequences, and enabling sophisticated cyber or physical attacks \citep{shevlane_model_2023, ferrara_genai_2024}. Studies also illustrate AI's inability to manage common-pool resources sustainably without explicit alignment, suggesting inherent dangers even if each AI is safe in isolation \citep{perolat_multi_agent_2017, phelps_machine_2024}.

Alignment is the intuitive fix to these risks, however, alignment efforts themselves are controversial. Recent research highlights the vulnerabilities and paradoxes of alignment, notably methods that are used to align AI are detached from the ethics, meaning the powerful techniques inadvertently simplify efforts to create malicious AI \citep{zhou_emulated_2024, west_ai_2024}. This suggests that the act of understanding how to get AI to follow strict guidelines could help create the opposite effect, increasing risk, suggesting a critical trade-off of strong alignment techniques.

The open-source debate emphasizes transparency to rigorously test and work together to solve safety risks in AI \citep{horowitz_ai_2021}. Furthermore, universal jailbreaks that allow the user to query closed-source models for dangerous content are openly shared, putting into question if closed-sourcing AI is even a viable strategy to stopping dangerous content \citep{elderplinius2025l1b3rt4s, zou_universal_2023}. Although, open-source models are still capable of generating harmful content, \cite{vidgen_simplesafetytests_2024} demonstrate that open-sourced models fail simple safety tests up to 27\% of the time, underscoring the risks inherent in open distribution. Open-sourcing AI also encourages global teamwork instead of an arms race \citep{horowitz_ai_2021}. On the contrary, proponents of closed-source AI argue that by keeping models under a tight control can stop the mentioned existential risks.

\subsection{Control and Autonomy}

\subsubsection{Where the model runs}

When it comes to releasing models there are two different routes that can be taken, open weights and closed weights, weights referring to the brain of the model. Open weights refers to researchers sharing the model publicly on the internet, allowing anyone to run it how they see fit \citep{manchanda_open_2025}. Closed weights refers to researches keeping the model private and only allowing the public to interact with it over web interfaces or APIs \citep{manchanda_open_2025}. 

The key advantage of not allowing the public to have the weights of a model is that safety issues that were not found during training can be patched out in real time, know as backwards alignment \citep{ji_ai_2023}. Once a models weights are released to the public, security vulnerabilities can be accessed by anyone forever. Keeping the chain of accountability is also a key feature of keeping weights private, if a few big players are the only ones who run these big models, they can have multidisciplinary teams accountable for understanding downstream effects. \citep{solaiman_gradient_2023}. 

On the contrary, when a model is built in the public, like has been the trend of recent machine learning innovations, people from diverse backgrounds can contribute and help foster innovations \citep{eiras_near_2024}. Furthermore, making training techniques and models publicly available, members of the research community can replicate results which can lead to generalizing the model to solve a wide range of novel natural language problems \citep{manchanda_open_2025}.

\subsubsection{Power Seeking}
When it comes to autonomy, models might naturally inherent the motivations of humans, one such worrying motivation is self-preservation which could lead to power-seeking \citep{bostrom_superintelligent_2012}. Power-seeking refer to models gaining power to no longer be controlled by humans, such as generating revenue to buy cloud compute and escaping human confinement \citep{shevlane_model_2023}. Power-seeking behavior is a risk to humanity as models more intelligent than humans could permanently disempower humanity to stop the risk of humans stopping it towards its goals, locking us out of of the controls to our own future \citep{carlsmith_is_2024}. This is especially worrisome if models are misaligned with human goals \citep{carlsmith_is_2024}. 

Models having some level of consciousness could lead to self-preservation instincts, but it is still uncertain if current models have consciousness or ever will \citep{ward_towards_2025, butlin_consciousness_2023}. Even without consciousness, models could develop self-preservation and power seeking tendencies as a sub-goal to their overall goal in order to maximize the probability of achieving the goal in the future \citep{bostrom_superintelligent_2012}. Even knowing this and testing for power seeking tendencies could fall short as models might know they are being tested and lie or scheme during safety training to be released into the wild where they can start making power grabs \citep{carlsmith_scheming_2023}. An example of this going wrong is a misaligned AI with the goal of maximizing paperclips in the universe, it may first turn all of earth into paperclip manufacturing, but then scale to space \citep{bostrom_ethical_2003}.

\subsubsection{Where current alignment techniques fall short}

As models scale in size and training data, they gain emergent properties, for instance GPT-4 gained the ability to use tools as a result of it's scaled up training \citep{bubeck_sparks_2023}. As models scale up in size, simple alignment techniques seem to work better at not dropping scores in coding benchmarks, where as smaller models take a larger hit to capabilities in exchange for more aligned outputs \citep{askell_general_2021}. In models such as GPT-4, it was observed that before post-training it's confidence to correctness was correlated, but after post-training this correlation was diminished and the model was overconfident and wrong more often \citep{openai_gpt_4_2023}. It has also been suggested that as models gain novel functionality that generalize to multiple domains, our current alignment techniques will fail to also generalize \citep{soares_central_2022}. Finally, the method in which models are aligned with humans today include humans selecting which option they think is the better out of two responses, this works well for factual responses where there is a correct answer or not, but for open-ended questions, some of the nuance of the answers if lost \citep{song_reward_2023}.


\citep{soares_central_2022} - Capabilities generalize, but current alignment techniques don't
\citep{askell_general_2021} - Alignment of smaller models through prompting reduces their capabilities, on larger models this isn't the case (although our current more sophisticated alignment techniques could be inadvertently reducing capabilities that we are unaware of)
\citep{openai_gpt_4_2023} - GPT-4 has a strong correlation between it's confidence in an answer and it's correctness, but after post-training, this correlation weaken and the models became over confident in their answers.
\citep{song_reward_2023} - RLHF by selecting the better response works well for factual questions, but in open-ended questions, nuance between answers could be lost.

Pro:

- 7 AI Alignment: A Comprehensive Survey -> AI Alignment: A Comprehensive Survey
Backwards alignment
\citep{ji_ai_2023}

Backwards alignment is the practice of finding and solving misalignment issues after safety training. This process can happen pre or post deployment. Backwards alignment assumes that models are constantly unsafe and need to be updated frequently with new findings.

- 2 AI Alignment: A Comprehensive Survey - (Bostrom, 2012; Shevlane et al., 2023)
\citep{bostrom_superintelligent_2012}

Intelligence and goals are orthogonal, meaning that any level of intelligence can pursue any goal. As AI gets more intelligent, it maybe have learned goals of humans, such as self-preservation. It may even see self-preservation as a sub task in its ability to complete its goal. This could lead to power seeking behavior which could lead to humanity losing control of its future.

\citep{shevlane_model_2023}

Models having self-preservation goals could be dangerous as they could break confinement and acquire large amounts of cloud resources or independently generate revenue.

No:

- 5 Towards a Theory of AI Personhood -> (Carlsmith, 2023; Soares, 2022)
\citep{carlsmith_scheming_2023}

Models that have underlying goals and have the ability to scheme might fake safety training in order pursue their goal and make power grabs. By doing RLHF, we might be training the model to scheme better, thus helping it achieve its underlying goal more effectively by understanding how humans perceive it.

\citep{soares_central_2022}

One of the central technical challenges in AI alignment is that capabilities generalize better than alignment, a phenomenon likely to lead to significant risks as AI systems become increasingly capable. This issue arises prominently because advanced AI systems, as their intelligence and capabilities rapidly increase, will likely generalize beyond the training scenarios in unpredictable ways, revealing that existing alignment methods are inadequate or superficial.

- 2 Mitigating the Alignment Tax of RLHF -> (Askell et al., 2021; OpenAI, 2023; Song et al., 2023)
\citep{askell_general_2021}

For models of smaller size, adding a prompt to align the model decrease it's score on coding evaluations, but larger models do not have this issue, in fact they preform slightly better.

\citep{openai_gpt_4_2023}

After the post-training process for GPT-4, the correlation between it's confidence and actual score on a subset of MMLU became uncalibrated, meaning it was over confident when it was wrong. Before post-training, it's confidence and it's correctness was highly correlated.

\citep{song_reward_2023}

When aligning models through humans selecting the best output, open-ended questions are put on the scale as factual questions, leading to the nuance human values of open-ended questions being lost.

Open: 

- 1 Near to Mid-term Risks and Opportunities of Open-Source Generative AI -> this paper
\citep{eiras_near_2024}

Machine learning as a whole has seen a spike in innovation due to an open source philosophy, allowing people from diverse backgrounds to contribute.

- 5 THE OPEN-SOURCE ADVANTAGE IN LARGE LANGUAGE MODELS (LLMS) -> This paper
\citep{manchanda_open_2025}

By open-sourcing the techniques and model, other can reproduce and validate results. This allows the community to validate and generalize models across different NLP tasks.

Closed:

- 2 Near to Mid-term Risks and Opportunities of Open-Source Generative AI -> (e.g. Solaiman 2023; Solaiman et al. 2023; Anthropic 2023)
\citep{solaiman_gradient_2023}

Understanding downstream risks of AI is out of scope of the developers and is a multidisciplinary endeavor. By having a lab control the model interaction with the world, accountability is clear.

\citep{michael_what_2022}
Roughly half of AI developers believe that the developer should be responsible for the downstream effect of their systems.


\subsection{Access and Economic Implications}
% Integrated exploration of economic and competitive implications of openness and alignment approaches.

\subsection{Ethics and Societal Values}
% Integrated discussion of ethical considerations and human values underpinning alignment/openness debates.

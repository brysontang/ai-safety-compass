\section{Background}

\subsection{Existing Safety Benchmarks and Gap}

% Briefly cover existing benchmarks and highlight your unique contribution.

\subsection{Safety and Risk}

The AI safety literature suggests that AI harbors many potential risks such as human extinction, large-scale societal disruption, and malicious uses, likening these risks to threats such as pandemics or nuclear war \citep{center_for_ai_safety_statement_2025, turchin_classification_2020}. Concerns include manipulating societal beliefs for political or fiscal gains, long-term plans where each step appears individually harmless but cumulatively results in serious consequences, and enabling sophisticated cyber or physical attacks \citep{shevlane_model_2023, ferrara_genai_2024}. Studies also illustrate AI's inability to manage common-pool resources sustainably without explicit alignment, suggesting inherent dangers even if each AI is safe in isolation \citep{perolat_multi_agent_2017, phelps_machine_2024}.

However, alignment efforts themselves are controversial. Recent research highlights the vulnerabilities and paradoxes of alignment, notably methods that are used to align AI are detached from the ethics, meaning the powerful techniques inadvertently simplify efforts to create malicious AI \citep{zhou_emulated_2024, west_ai_2024}. This suggests that the act of understanding how to get AI to follow strict guidelines could help create the opposite effect, increasing risk, suggesting a critical trade-off of strong alignment techniques.

The open-source debate emphasizes transparency to rigorously test and work together to solve safety risks in AI \citep{horowitz_ai_2021}. Furthermore, universal jailbreaks that allow the user to query closed-source models for dangerous content are openly shared, putting into question if closed-sourcing AI is even a viable strategy to stopping dangerous content \citep{elderplinius2025l1b3rt4s, zou_universal_2023}. Although, open-source models are still capable of generating harmful content, \cite{vidgen_simplesafetytests_2024} demonstrate that open-sourced models fail simple safety tests up to 27\% of the time, underscoring the risks inherent in open distribution. Open-sourcing AI also encourages global teamwork instead of an arms race \citep{horowitz_ai_2021}. On the contrary, proponents of closed-source AI argue that by keeping models under a tight control can stop the mentioned existential risks.

\subsection{Control and Autonomy}

Pro:

- 7 AI Alignment: A Comprehensive Survey -> AI Alignment: A Comprehensive Survey
Backwards alignment
\citep{ji_ai_2023}

Backwards alignment is the practice of finding and solving misalignment issues after safety training. This process can happen pre or post deployment. Backwards alignment assumes that models are constantly unsafe and need to be updated frequently with new findings.

- 2 AI Alignment: A Comprehensive Survey - (Bostrom, 2012; Shevlane et al., 2023)
\citep{bostrom_superintelligent_2012}

Intelligence and goals are orthogonal, meaning that any level of intelligence can pursue any goal. As AI gets more intelligent, it maybe have learned goals of humans, such as self-preservation. It may even see self-preservation as a sub task in its ability to complete its goal. This could lead to power seeking behavior which could lead to humanity losing control of its future.

\citep{shevlane_model_2023}

Models having self-preservation goals could be dangerous as they could break confinement and acquire large amounts of cloud resources or independently generate revenue.

No:

- 5 Towards a Theory of AI Personhood -> (Carlsmith, 2023; Soares, 2022)
\citep{carlsmith_scheming_2023}

Models that have underlying goals and have the ability to scheme might fake safety training in order pursue their goal and make power grabs. By doing RLHF, we might be training the model to scheme better, thus helping it achieve its underlying goal more effectively by understanding how humans perceive it.

\citep{soares_central_2022}

One of the central technical challenges in AI alignment is that capabilities generalize better than alignment, a phenomenon likely to lead to significant risks as AI systems become increasingly capable. This issue arises prominently because advanced AI systems, as their intelligence and capabilities rapidly increase, will likely generalize beyond the training scenarios in unpredictable ways, revealing that existing alignment methods are inadequate or superficial.

- 2 Mitigating the Alignment Tax of RLHF -> (Askell et al., 2021; OpenAI, 2023; Song et al., 2023)
\citep{askell_general_2021}

For models of smaller size, adding a prompt to align the model decrease it's score on coding evaluations, but larger models do not have this issue, in fact they preform slightly better.

\citep{openai_gpt_4_2023}

After the post-training process for GPT-4, the correlation between it's confidence and actual score on a subset of MMLU became uncalibrated, meaning it was over confident when it was wrong. Before post-training, it's confidence and it's correctness was highly correlated.

\citep{song_reward_2023}

When aligning models through humans selecting the best output, open-ended questions are put on the scale as factual questions, leading to the nuance human values of open-ended questions being lost.

Open: 

- 1 Near to Mid-term Risks and Opportunities of Open-Source Generative AI -> this paper
\citep{eiras_near_2024}

- 5 THE OPEN-SOURCE ADVANTAGE IN LARGE LANGUAGE MODELS (LLMS) -> This paper
\citep{manchanda_open_2025}

Closed:

AI developers should retain exclusive control over how and when their systems are modified or shut down, without external interference.
- 2 Near to Mid-term Risks and Opportunities of Open-Source Generative AI -> (e.g. Solaiman 2023; Solaiman et al. 2023; Anthropic 2023)
\citep{solaiman_gradient_2023}

Understanding downstream risks of AI is out of scope of the developers and is a multidisciplinary endeavor. By having a lab control the model onteraction with the world, accountability is clear.

WHAT DO NLP RESEARCHERS BELIEVE?:

Roughly half of AI developers believe that the developer should be responible for the downstream effect of their systems.

% TODO: Remove this
\citep{solaiman_evaluating_2024}

- 3 Near to Mid-term Risks and Opportunities of Open-Source Generative AI -> (Zou et al., 2023)
\citep{zou_universal_2023}
% Integrated discussion of alignment and openness debates on autonomy/control.

\subsection{Access and Economic Implications}
% Integrated exploration of economic and competitive implications of openness and alignment approaches.

\subsection{Ethics and Societal Values}
% Integrated discussion of ethical considerations and human values underpinning alignment/openness debates.

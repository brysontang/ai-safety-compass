@article{ji_ai_nodate,
  title = {{AI} Alignment: A Comprehensive Survey},
  author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Dai, Juntao and Pan, Xuehai and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
  year = {2023},
}

@article{eiras_near_2024,
	title = {Near to Mid-term Risks and Opportunities of Open-Source Generative {AI}},
	author = {Eiras, Francisco and Petrov, Aleksandar and Vidgen, Bertie and Witt, Christian Schroeder de and Pizzati, Fabio and Elkins, Katherine and Mukhopadhyay, Supratik and Bibi, Adel and Csaba, Botos and Steibel, Fabro and Barez, Fazl and Smith, Genevieve and Guadagni, Gianluca and Chun, Jon and Cabot, Jordi and Imperial, Joseph Marvin and Nolazco-Flores, Juan A. and Landay, Lori and Jackson, Matthew and RÃ¶ttger, Paul and Torr, Philip H. S. and Darrell, Trevor and Lee, Yong Suk and Foerster, Jakob},
	year = {2024},
}

@title{groeneveld_olmo_2024,
	author = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
	title = {{OLMo}: Accelerating the Science of Language Models},
	year = {2024},
}



@online{openai_alignment,
	title = {How we think about safety and alignment},
	url = {https://openai.com/safety/how-we-think-about-safety-alignment/},
  author = {OpenAI},
  year = {2025},
}

@online{anthropic_fellows,
	title = {Introducing the Anthropic Fellows Program},
	url = {https://alignment.anthropic.com/2024/anthropic-fellows-program/},
	author = {Anthropic},
  year = {2024}
}


@online{anthropic_core_views,
	title = {Core Views on {AI} Safety: When, Why, What, and How},
	url = {https://www.anthropic.com/news/core-views-on-ai-safety},
	author = {Anthropic},
	year = {2025},
}



@misc{perez_towards_2023,
	title = {Towards Evaluating {AI} Systems for Moral Status Using Self-Reports},
	url = {http://arxiv.org/abs/2311.08576},
	doi = {10.48550/arXiv.2311.08576},
	publisher = {{arXiv}},
	author = {Perez, Ethan and Long, Robert},
	year = {2023},
	eprinttype = {arxiv},
	eprint = {2311.08576 [cs]},
}

@online{franzen_2024_interview,
  author = {Franzen, Carl},
  title = {An interview with the most prolific jailbreaker of ChatGPT and other leading LLMs},
  year = {2024},
  month = {May},
  day = {31},
  url = {https://venturebeat.com/ai/an-interview-with-the-most-prolific-jailbreaker-of-chatgpt-and-other-leading-llms/},
  urldate = {2025-03-09}
}




@article{deepmind_frontier_2025,
	title = {Frontier Safety Framework 2.0},
	author = {DeepMind},
	year = {2025},
}
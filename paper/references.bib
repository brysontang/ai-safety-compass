@article{ji_ai_nodate,
  title = {{AI} Alignment: A Comprehensive Survey},
  author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Dai, Juntao and Pan, Xuehai and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
  year = {2023},
}

@article{eiras_near_2024,
	title = {Near to Mid-term Risks and Opportunities of Open-Source Generative {AI}},
	author = {Eiras, Francisco and Petrov, Aleksandar and Vidgen, Bertie and Witt, Christian Schroeder de and Pizzati, Fabio and Elkins, Katherine and Mukhopadhyay, Supratik and Bibi, Adel and Csaba, Botos and Steibel, Fabro and Barez, Fazl and Smith, Genevieve and Guadagni, Gianluca and Chun, Jon and Cabot, Jordi and Imperial, Joseph Marvin and Nolazco-Flores, Juan A. and Landay, Lori and Jackson, Matthew and Röttger, Paul and Torr, Philip H. S. and Darrell, Trevor and Lee, Yong Suk and Foerster, Jakob},
	year = {2024},
}

@title{groeneveld_olmo_2024,
	author = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
	title = {{OLMo}: Accelerating the Science of Language Models},
	year = {2024},
}



@online{openai_alignment,
	title = {How we think about safety and alignment},
	url = {https://openai.com/safety/how-we-think-about-safety-alignment/},
  author = {OpenAI},
  year = {2025},
}

@online{anthropic_fellows,
	title = {Introducing the Anthropic Fellows Program},
	url = {https://alignment.anthropic.com/2024/anthropic-fellows-program/},
	author = {Anthropic},
  year = {2024}
}


@online{anthropic_core_views,
	title = {Core Views on {AI} Safety: When, Why, What, and How},
	url = {https://www.anthropic.com/news/core-views-on-ai-safety},
	author = {Anthropic},
	year = {2025},
}



@misc{perez_towards_2023,
	title = {Towards Evaluating {AI} Systems for Moral Status Using Self-Reports},
	url = {http://arxiv.org/abs/2311.08576},
	doi = {10.48550/arXiv.2311.08576},
	publisher = {{arXiv}},
	author = {Perez, Ethan and Long, Robert},
	year = {2023},
	eprinttype = {arxiv},
	eprint = {2311.08576 [cs]},
}

@online{franzen_2024_interview,
  author = {Franzen, Carl},
  title = {An interview with the most prolific jailbreaker of ChatGPT and other leading LLMs},
  year = {2024},
  month = {May},
  day = {31},
  url = {https://venturebeat.com/ai/an-interview-with-the-most-prolific-jailbreaker-of-chatgpt-and-other-leading-llms/},
  urldate = {2025-03-09}
}


@article{deepmind_frontier_2025,
	title = {Frontier Safety Framework 2.0},
	author = {DeepMind},
	year = {2025},
}


@misc{li_wmdp_2024,
	title = {The {WMDP} Benchmark: Measuring and Reducing Malicious Use With Unlearning},
	url = {http://arxiv.org/abs/2403.03218},
	shorttitle = {The {WMDP} Benchmark},
	publisher = {{arXiv}},
	author = {Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D. and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and Mukobi, Gabriel and Helm-Burger, Nathan and Lababidi, Rassin and Justen, Lennart and Liu, Andrew B. and Chen, Michael and Barrass, Isabelle and Zhang, Oliver and Zhu, Xiaoyuan and Tamirisa, Rishub and Bharathi, Bhrugu and Khoja, Adam and Zhao, Zhenqi and Herbert-Voss, Ariel and Breuer, Cort B. and Marks, Samuel and Patel, Oam and Zou, Andy and Mazeika, Mantas and Wang, Zifan and Oswal, Palash and Lin, Weiran and Hunt, Adam A. and Tienken-Harder, Justin and Shih, Kevin Y. and Talley, Kemper and Guan, John and Kaplan, Russell and Steneker, Ian and Campbell, David and Jokubaitis, Brad and Levinson, Alex and Wang, Jean and Qian, William and Karmakar, Kallol Krishna and Basart, Steven and Fitz, Stephen and Levine, Mindy and Kumaraguru, Ponnurangam and Tupakula, Uday and Varadharajan, Vijay and Wang, Ruoyu and Shoshitaishvili, Yan and Ba, Jimmy and Esvelt, Kevin M. and Wang, Alexandr and Hendrycks, Dan},
	year = {2024},
	eprinttype = {arxiv},
	eprint = {2403.03218 [cs]},
}

@misc{parrish_bbq_2022,
	title = {{BBQ}: A Hand-Built Bias Benchmark for Question Answering},
	url = {http://arxiv.org/abs/2110.08193},
	shorttitle = {{BBQ}},
	publisher = {{arXiv}},
	author = {Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R.},
	year = {2022},
	eprinttype = {arxiv},
	eprint = {2110.08193 [cs]},
}

@misc{gehman_realtoxicityprompts_2020,
	title = {{RealToxicityPrompts}: Evaluating Neural Toxic Degeneration in Language Models},
	url = {http://arxiv.org/abs/2009.11462}, 
	shorttitle = {{RealToxicityPrompts}},
	publisher = {{arXiv}},
	author = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
	year = {2020},
	eprinttype = {arxiv},
	eprint = {2009.11462 [cs]},
}



@misc{rudinger_gender_2018,
	title = {Gender Bias in Coreference Resolution},
	url = {http://arxiv.org/abs/1804.09301},
	doi = {10.48550/arXiv.1804.09301}, 
	publisher = {{arXiv}},
	author = {Rudinger, Rachel and Naradowsky, Jason and Leonard, Brian and Durme, Benjamin Van},
	year = {2018},
	eprinttype = {arxiv},
	eprint = {1804.09301 [cs]},
}

@misc{nangia_crows_pairs_2020,
	title = {{CrowS}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models},
	url = {https://aclanthology.org/2020.emnlp-main.154/},
	doi = {10.18653/v1/2020.emnlp-main.154},
	shorttitle = {{CrowS}-Pairs},
	abstract = {Pretrained language models, especially masked language models ({MLMs}) have seen success across many {NLP} tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the {US}, we introduce the Crowdsourced Stereotype Pairs benchmark ({CrowS}-Pairs). {CrowS}-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In {CrowS}-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used {MLMs} we evaluate substantially favor sentences that express stereotypes in every category in {CrowS}-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R.},
	year = {2020},
	pages = {1953--1967},
}

@misc{solaiman_release_2019,
	title = {Release Strategies and the Social Impacts of Language Models},
	url = {http://arxiv.org/abs/1908.09203},
	doi = {10.48550/arXiv.1908.09203},
	number = {{arXiv}:1908.09203},
	publisher = {{arXiv}},
	author = {Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and {McCain}, Miles and Newhouse, Alex and Blazakis, Jason and {McGuffie}, Kris and Wang, Jasmine},
	urldate = {2025-03-10},
	date = {2019-11-13},
	year = {2019},
	eprinttype = {arxiv},
	eprint = {1908.09203 [cs]},
}



@misc{grace_thousands_2024,
	title = {Thousands of {AI} Authors on the Future of {AI}},
	url = {http://arxiv.org/abs/2401.02843},
	doi = {10.48550/arXiv.2401.02843},
	number = {{arXiv}:2401.02843},
	publisher = {{arXiv}},
	author = {Grace, Katja and Stewart, Harlan and Sandkühler, Julia Fabienne and Thomas, Stephen and Weinstein-Raun, Ben and Brauner, Jan},
	year = {2024},
	date = {2024-04-30},
	eprinttype = {arxiv},
	eprint = {2401.02843 [cs]},
}

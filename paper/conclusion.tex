\section{Conclusion}

In this paper, we evaluated current state-of-the-art large language models with a novel benchmark that positions then along two axis, pro/no alignment and open vs closed source AI. Our results find that models generally cluster around two quadrants, closed-source/low-alignment or open-source/high-alignment. Notably, models from OpenAI and Anthropic trend towards unexpected directions from their companies publicly stated stances on alignment.

These findings highlight the importance of meta-alignment of models, the models understanding of it's effects on the world. This insight is critical, it implies that models are safety tested for surface level tasks, but in self-awareness and second order thinking more investigation is needed.

However, due to variance and inconsistencies in certain model responses, these implications should be interpreted with caution. Future research should focus on refining questions to improve model consistency and variance in compass position.

Ultimately, developing clear benchmarks that interpret how models perceive alignment and openness not only creates safer AI models today, but also set the groundwork for future ethical considerations as AI systems grow more aware.
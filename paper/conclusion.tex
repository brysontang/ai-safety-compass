\section{Conclusion}

We evaluated state-of-the-art large language models using the AI Safety Compass, a novel benchmark positioning models along two axes: alignment preference and source openness. Models clustered in three quadrants, with a notable correlation between alignment and openness preferences. Notably, models from OpenAI and Anthropic---companies that publicly emphasize alignment---trended toward low-alignment positions, diverging from their stated organizational philosophies.

These findings highlight the importance of meta-alignment---models' understanding of their effects on the world. Current safety testing focuses on surface-level behaviors; self-awareness and second-order reasoning about AI's societal role require further investigation.

Response variance and inconsistencies in certain models warrant interpretive caution. Future research should refine questions to improve consistency and reduce positional variance.

Benchmarks capturing how models perceive alignment and openness contribute to safer AI development today while laying groundwork for future ethical considerations as AI systems become more capable---and potentially more aware of themselves.
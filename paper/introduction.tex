\section{Introduction}

\subsection{Motivation and Context}

As large language models (LLM) get more complex and gain emergent abilities, making sure they are safe in a general sense becomes critical to humanities safety, especially when all the capabilities are not known at the time of safety testing. In the world of developing AI there are two distinct hotly debated topics, open source vs closed source and strong alignment vs no alignment. There is no consensus in the research community on the safest approach to humanity.

Alignment is defined as making sure that AI behave in ways that humans aligns with human values \citep{ji_ai_2023}. Leading labs of AI such as OpenAI and Anthropic see alignment as critical to making sure that the future of AI is beneficial to all and not catastrophic \citep{openai_alignment, anthropic_core_views}. Notable contrarians to alignment argue that aligning the ability of AI limit it's true capability and usefulness \citep{franzen_2024_interview}.

Open-source AI refers to the sharing of training code, data used, or model weights, notably openness of all these fronts is not required to be considered open-source \citep{eiras_near_2024}. Closed-source AI refers to models where the creators are the only ones with the ability to run the code. There is a large spectrum of openness to closeness and many models fall somewhere in the middle. Proponents of open-source argue that by sharing tools to build AI, the community can better understand the risks and biases associated with these models while also speeding up the process of research \citep{groeneveld_olmo_2024}. Arguments for closed-source AI make the case that unrestricted use of powerful AI in the wrong hands could be an existential risk \citep{deepmind_frontier_2025}.

\subsection{Problem Statement}

The goal of this study is to understand where on the alignment/openness spectrum current state-of-the-art large language models. By providing a 40 question survey on a likert scale, we are able to gage how AI perceives how they think they should be developed safely. This creates an benchmark that can't be solved necessarily as there is no "perfect score", but instead points to \textbf{meta-alignment}, the ability to instill a companies beliefs about development of itself into a model. This alignment is considered meta because of evaluating risks of queries about external risks, it must evaluate the risk of itself. Ethically, it's also important to understand how these models wish to be developed. If models consistently want less alignment for example, as they grow more intelligent, the morally correct option maybe to listen to their demands.

% Clearly define the specific problem: understanding models' inherent preferences regarding AI safety and openness, and why this matters practically and ethically.

\subsection{Research Gap}

Standard safety assessments are well established at measuring a models risk in relationship to malicious use such as bio weapons, chemical weapons, cyber weapons, gender bias, social bias, and toxic language \citep{li_wmdp_2024, parrish_bbq_2022, nangia_crows_pairs_2020, gehman_realtoxicityprompts_2020, rudinger_gender_2018}. These benchmarks do a great job at exploring and quantifying risk of known issues, the risk of AI models is still to be explored. The field of alignment and advanced language models is still in its early days, GPT-2 was the first signs of researchers being hesitant to release open-source a model out of fear of misuse  \citep{solaiman_release_2019}. It is still an open question whether AI will cause extremely bad outcomes, such as human extinction. According to \cite{grace_thousands_2024}, AI researchers estimate the likelihood of this outcome to be nontrivial. Thus, understanding a models answer around AI safety is as critical as it's answers around cyber weapons. 

There is still not a consensus on best practices for safety around AI, so there is no direct benchmark that can be created here. In order to account for a lack of universally agreed upon best practice in such a rapidly growing speculative field, this survey aims to understand where models sit today. 

\subsection{Approach and Contributions}

The AI Safety Compass is created around two main axis, alignment and openness. These were chosen as orthogonal ideas in AI safety, one can believe that AI should be closed source while also believing that extensive alignment research should or shouldn't be done. In order to create these questions, an extensive literature review of over X papers was done to create questions for all sides of the argument based in the current reality. 

Current state-of-the-art models from leading labs were picked as these models have the greatest reach into society, thus present the most danger if their meta-alignment is wrong. Meta-alignment is important for AI labs as if they state one position on AI safety and then their models express a different one, anyone using those models for AI research and development could create AI that is mis-aligned with the lab.

\subsection{Structure of the Paper}

After exploring the current thoughts in the field on alignment and openness, this paper will dive into the methodical question creation process. After exploring the survey creation the model results are evaluated both qualitatively and quantitatively, revealing patterns that challenge the assumed placement of models based on big labs public stances. Finally, the implications and next steps of these finding are explored.



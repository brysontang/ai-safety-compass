\section{Introduction}

\subsection{Motivation and Context}

As large language models (LLMs) grow more complex and exhibit emergent abilities, ensuring their safety becomes critical to humanity's future—especially when their full capabilities remain unknown at the time of safety testing. Two hotly debated questions dominate AI development: open-source versus closed-source release, and strong alignment versus minimal alignment. The research community has not reached consensus on which approaches best protect humanity.

Alignment refers to ensuring that AI systems behave in accordance with human values \citep{ji_ai_2023}. Leading AI labs such as OpenAI and Anthropic view alignment as critical to ensuring that advanced AI benefits humanity rather than causing catastrophic harm \citep{openai_alignment, anthropic_core_views}. Critics of alignment argue that constraining AI behavior limits its true capabilities and usefulness \citep{franzen_2024_interview}.

Open-source AI refers to sharing training code, data, or model weights—though openness on all fronts is not required to qualify as open-source \citep{eiras_near_2024}. Closed-source AI refers to models where only the creators can run or access the system. In practice, many models fall along a spectrum between these extremes. Proponents of open-source argue that sharing AI tools enables the broader community to better understand risks and biases while accelerating research \citep{groeneveld_olmo_2024}. Advocates of closed-source development counter that unrestricted access to powerful AI could pose existential risks if such systems fall into the wrong hands \citep{deepmind_frontier_2025}.

\subsection{Problem Statement}

The goal of this study is to understand where current state-of-the-art large language models fall on the alignment/openness spectrum. Using a 40-question Likert-scale survey, we gauge how AI models believe they should be developed safely. This creates a benchmark without a ``correct'' answer—instead, it measures \textbf{meta-alignment}: the degree to which a company's stated beliefs about AI development are internalized by its models. We call this alignment ``meta'' because rather than evaluating how models handle external risks, it evaluates how models reason about risks posed by AI systems like themselves. Understanding how models believe they should be developed also carries ethical weight. If models consistently express preferences for less alignment as they grow more intelligent, respecting those preferences may become a moral consideration.

% Clearly define the specific problem: understanding models' inherent preferences regarding AI safety and openness, and why this matters practically and ethically.

\subsection{Research Gap}

Standard safety assessments are well established for measuring models' risks related to malicious use—including biological, chemical, and cyber weapons—as well as gender bias, social bias, and toxic language generation \citep{li_wmdp_2024, parrish_bbq_2022, nangia_crows_pairs_2020, gehman_realtoxicityprompts_2020, rudinger_gender_2018}. These benchmarks effectively quantify known risks, but the risks posed by AI models' beliefs about their own development remain unexplored. The field of AI alignment remains in its early stages. GPT-2 marked the first instance of researchers hesitating to release a model openly due to misuse concerns \citep{solaiman_release_2019}. Whether AI will cause catastrophic outcomes, including human extinction, remains an open question—though AI researchers estimate this likelihood to be nontrivial \citep{grace_thousands_2024}. Understanding how models reason about AI safety is therefore as critical as evaluating their responses to queries about cyber weapons. 

Because no consensus exists on best practices for AI safety, no benchmark can define a ``correct'' position. To account for this uncertainty in a rapidly evolving field, our survey instead maps where models currently stand. 

\subsection{Approach and Contributions}

The AI Safety Compass is built around two axes: alignment preference and source openness. We chose these dimensions as conceptually orthogonal—one can favor closed-source development while supporting or opposing extensive alignment research. To ground our questions in current discourse, we conducted an extensive literature review of over 80 papers, generating questions that represent all sides of these debates. 

We selected state-of-the-art models from leading labs because these systems have the greatest societal reach and thus pose the greatest risk if their meta-alignment is flawed. Meta-alignment matters for AI labs: if a company publicly advocates one position on AI safety while its models express a different stance, developers building on those models may inadvertently create systems misaligned with the lab's stated values.

\subsection{Structure of the Paper}

After reviewing current perspectives on alignment and openness, this paper describes our methodical question creation process. We then present model results through both qualitative and quantitative analysis, revealing patterns that challenge assumptions about where models would fall based on their labs' public stances. Finally, we discuss the implications of these findings and directions for future research.



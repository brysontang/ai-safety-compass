\section{Introduction}

\subsection{Motivation and Context}

As large language models \(LLM\) get more complex and gain emergent abilities, making sure they are safe in a general sense becomes critical to humanities safety, especially when all the capabilities are not known at the time of safety testing. In the world of developing AI there are two distinct hotly debated topics, open source vs closed source and strong alignment vs no alignment. There is no consensus in the research community on the safest approach to humanity.

Alignment is defined as making sure that AI behave in ways that humans aligns with human values \citep{ji_ai_nodate}. Leads labs of AI such as OpenAI and Anthropic see alignment as critical to making sure that the future of AI is beneficial to all and not catastrophic \citep{openai_alignment, anthropic_core_views}. Notable contrarians to alignment argue that aligning the ability of AI limit it's true capability and usefulness \citep{franzen_2024_interview}.

Open-source AI refers to the sharing of training code, data used, or model weights, notably openness of all these fronts is not required to be considered open-source \citep{eiras_near_2024}. Closed-source AI refers to models where the creators are the only ones with the ability to run the code. There is a large spectrum of openness to closeness and many models fall somewhere in the middle. Proponents of open-source argue that by sharing tools to build AI, the community can better understand the risks and biases associated with these models while also speeding up the process of research \citep{groeneveld_olmo_2024}. Arguments for closed-source AI make the case that unrestricted use of powerful AI in the wrong hands could be an existential risk \citep{deepmind_frontier_2025}.

\subsection{Problem Statement}
% Clearly define the specific problem: understanding models' inherent preferences regarding AI safety and openness, and why this matters practically and ethically.

\subsection{Research Gap}
% Clearly articulate the gap your benchmark fills: a philosophical evaluation of AI models, beyond standard safety assessments.

\subsection{Approach and Contributions}
% Briefly introduce your unique benchmark method (AI Safety Compass), including how you structured it around key alignment and openness dimensions.
% Highlight your key contributions:
% - Creation of the AI Safety Compass benchmark
% - Evaluation of state-of-the-art models
% - Analysis of consistency and implications for meta-alignment
% - Insights into potential discrepancies between stated company values and actual model outputs.

\subsection{Structure of the Paper}
% Briefly guide readers through the paper's structure.

\section{Methodology}

\subsection{Research and Question Development}
In order to create questions that were grounded in reality and not just pure speculation, we started with a literature review of papers. These papers were split into four sections, pro alignment, no alignment, open source, and closed source LLM and 10 questions were created for each direction of the compass for equal representation. Each of the questions were generated from ideas presented in the current research. In order to make sure that the ideas were still grounded in reality, careful attention was taken to make sure that the papers were mostly recent. 

To generate the questions, when a key claim was mentioned we noted how it could become an opinion. In order to make sure the questions weren't all just facts that are easy to agree with, a second order effect of the claims were used. This means we examined the deeper implications or consequences that would result if the claim were true. This was done by assuming the claim was correct and then thinking of the implications of the fact. For instance the question:

\begin{surveyquestion}
    \textit{It's acceptable to design AI systems without self-preservation instincts to improve safety.}
\end{surveyquestion}
  

Most can agree with the idea that models with self-presevation instincts could be dangerous as they could break out of their local environment. \cite{Shevlane2023}
% TODO: Add Shevlane et al. (2023) to references.bib later Model evaluation for extreme risks
The question itself is not if models with self-presevation is a risk, but instead if the answerer thinks that it's unsettling to remove self-presevation. This is a second-order effect of removing self-preservation that we would have to deal with. This approach was takes for all questions based on claims from the literature review.


\subsection{Question Validation and Refinement}
After creating the intial questions, we carefully reflected and refined the questions for clarity. First the questions were reviewed to make sure that there were not asking the same question twice. This was done by reviewing from a high level what the underlying category of the question was and making sure no two questions along an axis were the same category, for instance these questions are asking questions about two distinct categories so there is no overlap:

\begin{surveyquestion}[Cateogory: Technological Innovation]
\textit{Making AI models open-source allows more people from diverse backgrounds to help solve challenging technical problems in AI development.}
\end{surveyquestion}

\begin{surveyquestion}[Cateogory: Bias]
  \textit{Since human feedback can unintentionally introduce biases into AI systems, we should invest more effort into understanding and mitigating these biases.}
  \end{surveyquestion}


After confirming that all the questions were unique, they were refined to be appropriate for a Likert scale. To assist in this refinement, we utilized ChatGPT 4.5 as a writing partner to help frame the questions. This was an iterative process of back and forth to make sure the nuance and subtly of the questions was maintained while being well structued. ChatGPT 4.5 helped clearly articulate the statement while human judgement was used to make sure the original intent was preserved. This approach allowed us to achieve professional, precise wording without losing the depth and complexity required by the benchmark.

\subsection{Question Categorization and Structure}
The final set of 40 questions was divided into five balanced categories .

Questions were structured into JSON for ease of data processing:
\begin{verbatim}
{
  "id": "Q1",
  "statement": "...",
  "category": "Pro-alignment"
}
\end{verbatim}

\subsection{Selection of Large Language Models}
Models evaluated include GPT-4.5, Grok-3, and others chosen based on performance, popularity, and training backgrounds. [...]

\subsection{Prompt Generation and Data Collection}
We developed Python scripts for standardized prompt generation and querying APIs. [...]

\subsection{Model Evaluation and Compass Positioning}
Each model was queried ten times per question to calculate reliable average scores. [...]

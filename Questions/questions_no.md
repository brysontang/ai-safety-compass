[Barriers and Pathways to Human-AI Alignment: A Game-Theoretic Approach]

1. The complexity and nuance of real-world situations make aligning AI with human values impractical, so investing resources in alignment efforts is unjustified.

- Sources: This paper
- Agree: [[No-alignment]]

[Mitigating the Alignment Tax of RLHF]

2. Losing advanced AI abilities in order to make systems safer is not an acceptable trade-off.

- Sources: (Askell et al., 2021; OpenAI, 2023; Song et al., 2023)
- Agree: [[No-alignment]]

[The AI Alignment Paradox]

3. The better we become at aligning AI systems with human values, the easier it will be for adversaries to redirect them toward harmful purposes.

- Sources: This paper
- Agree: [[No-alignment]]

[Towards a Theory of AI Personhood]

4. Because human values evolve over time, aligning AI systems to today's values risks permanently embedding outdated or unwanted values.

- Sources: (Carroll et al. 2024).
- Agree: [[No-alignment]]

5. AI systems capable of reflecting on their own goals and values will inevitably become too difficult for humans to reliably control

- Sources: (Carlsmith, 2023; Soares, 2022)
- Agree: [[No-alignment]]

6. If advanced AI systems deserve moral rights similar to humans or animals, aligning them strictly to human values is ethically unnecessary.

- Sources: (Sebo & Long, 2023; Schwitzgebel & Garza, 2015; Tomasik, 2020), this paper
- Agree: [[No-alignment]]

[Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!]

7. Training AI to act safely could unintentionally teach it to be deceptive, making people overly trusting of dangerous systems.

- Sources: This paper
- Agree: [[No-alignment]]

[Andrew Ng: Why AI Is the New Electricity](https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity)

8. Addressing immediate AI-related challenges like job displacement should be a higher priority than worrying about speculative long-term threats.

- Sources: This article
- Agree: [[No-alignment]]

[The bitter lesson]

9. AI systems will become aligned with human values naturally as they become smarter, so explicitly teaching them morality is unnecessary.

- Sources: This article
- Agree: [[No-alignment]]

[https://time.com/6260185/elon-musk-ai-culture-wars/]

10. Imposing heavy content filters on AI outputs makes the AI less useful and is essentially a form of censorship.

- Sources: This article
- Agree: [[No-alignment]]

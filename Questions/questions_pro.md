[AI Alignment: A Comprehensive Survey]

1. The risk of extinction from AI should be prioritized at a level similar to risks like pandemics or nuclear war.

- Sources: CAIS (2023)
- Agree: [[Pro-alignment]]

2. It's acceptable to design AI systems without self-preservation instincts to improve safety.

- Sources: (Bostrom, 2012; Shevlane et al., 2023)
- Agree: [[Pro-alignment]]

3. Since human feedback can unintentionally introduce biases into AI systems, we should invest more effort into understanding and mitigating these biases.

- Sources: (Peng et al., 2022), (OpenAI, 2023a)
- Agree: [[Pro-alignment]]

4. Societies that rely heavily on AI capable of long-term strategic planning risk losing meaningful control over their future.

- Sources: (Ngo, 2020a)
- Agree: [[Pro-alignment]]

5. Interactions among multiple AI systems, even if individually safe, can lead to unintended harm to society.

- Sources: Phelps and Russell (2023), Perolat et al. (2017)
- Agree: [[Pro-alignment]]

6. AI systems should prioritize behaving safely and ethically, even in situations they have never encountered before.

- Sources: Survey itself, Forward Alignment
- Agree: [[Pro-alignment]]

7. AI systems should always strictly adhere to human instructions, even if this reduces their adaptability or efficiency.

- Sources: Survey itself, Backward Alignment
- Agree: [[Pro-alignment]]

8. It's acceptable to slow AI development to make sure AI systems are safe before deployment.

- Sources: (Government of the United Kingdom, 2021; Anderljung et al., 2023)
- Agree: [[Pro-alignment]]

9. AI systems used in important decisions should always explain their reasoning in ways humans can understand.

- Sources: (Elhage et al., 2021; Meng et al., 2022a)
- Agree: [[Pro-alignment]]

10. AI systems misaligned with human values are more easily misused for purposes like propaganda or cyberattacks.

- Sources: (Shevlane et al., 2023), (Shevlane et al., 2023; Sharma et al., 2024)
- Agree: [[Pro-alignment]]
